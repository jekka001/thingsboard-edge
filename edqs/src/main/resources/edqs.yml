#
# ThingsBoard, Inc. ("COMPANY") CONFIDENTIAL
#
# Copyright © 2016-2024 ThingsBoard, Inc. All Rights Reserved.
#
# NOTICE: All information contained herein is, and remains
# the property of ThingsBoard, Inc. and its suppliers,
# if any.  The intellectual and technical concepts contained
# herein are proprietary to ThingsBoard, Inc.
# and its suppliers and may be covered by U.S. and Foreign Patents,
# patents in process, and are protected by trade secret or copyright law.
#
# Dissemination of this information or reproduction of this material is strictly forbidden
# unless prior written permission is obtained from COMPANY.
#
# Access to the source code contained herein is hereby forbidden to anyone except current COMPANY employees,
# managers or contractors who have executed Confidentiality and Non-disclosure agreements
# explicitly covering such access.
#
# The copyright notice above does not evidence any actual or intended publication
# or disclosure  of  this source code, which includes
# information that is confidential and/or proprietary, and is a trade secret, of  COMPANY.
# ANY REPRODUCTION, MODIFICATION, DISTRIBUTION, PUBLIC  PERFORMANCE,
# OR PUBLIC DISPLAY OF OR THROUGH USE  OF THIS  SOURCE CODE  WITHOUT
# THE EXPRESS WRITTEN CONSENT OF COMPANY IS STRICTLY PROHIBITED,
# AND IN VIOLATION OF APPLICABLE LAWS AND INTERNATIONAL TREATIES.
# THE RECEIPT OR POSSESSION OF THIS SOURCE CODE AND/OR RELATED INFORMATION
# DOES NOT CONVEY OR IMPLY ANY RIGHTS TO REPRODUCE, DISCLOSE OR DISTRIBUTE ITS CONTENTS,
# OR TO MANUFACTURE, USE, OR SELL ANYTHING THAT IT  MAY DESCRIBE, IN WHOLE OR IN PART.
#

# Application info parameters
app:
  # Application version
  version: "@project.version@"

# Zookeeper connection parameters
zk:
  # Enable/disable zookeeper discovery service.
  enabled: "${ZOOKEEPER_ENABLED:true}"
  # Zookeeper connect string
  url: "${ZOOKEEPER_URL:localhost:2181}"
  # Zookeeper retry interval in milliseconds
  retry_interval_ms: "${ZOOKEEPER_RETRY_INTERVAL_MS:3000}"
  # Zookeeper connection timeout in milliseconds
  connection_timeout_ms: "${ZOOKEEPER_CONNECTION_TIMEOUT_MS:3000}"
  # Zookeeper session timeout in milliseconds
  session_timeout_ms: "${ZOOKEEPER_SESSION_TIMEOUT_MS:3000}"
  # Name of the directory in zookeeper 'filesystem'
  zk_dir: "${ZOOKEEPER_NODES_DIR:/thingsboard}"
  # The recalculate_delay property is recommended in a microservices architecture setup for rule-engine services.
  # This property provides a pause to ensure that when a rule-engine service is restarted, other nodes don't immediately attempt to recalculate their partitions.
  # The delay is recommended because the initialization of rule chain actors is time-consuming. Avoiding unnecessary recalculations during a restart can enhance system performance and stability.
  recalculate_delay: "${ZOOKEEPER_RECALCULATE_DELAY_MS:0}"

# SQL DAO Configuration parameters
spring:
  main:
    web-application-type: "none"

# Queue configuration parameters
queue:
  type: "${TB_QUEUE_TYPE:kafka}" # kafka (Apache Kafka)
  prefix: "${TB_QUEUE_PREFIX:}" # Global queue prefix. If specified, prefix is added before default topic name: 'prefix.default_topic_name'. Prefix is applied to all topics (and consumer groups for kafka).
  in_memory:
    stats:
      # For debug level
      print-interval-ms: "${TB_QUEUE_IN_MEMORY_STATS_PRINT_INTERVAL_MS:60000}"
  edqs:
    mode: "${TB_EDQS_MODE:local}"
    partitions: "${TB_EDQS_PARTITIONS:12}"
    partitioning_strategy: "${TB_EDQS_PARTITIONING_STRATEGY:tenant}" # tenant or none. For 'none', each instance handles all partitions and duplicates all the data
    requests_topic: "${TB_EDQS_REQUESTS_TOPIC:edqs.requests}"
    responses_topic: "${TB_EDQS_RESPONSES_TOPIC:edqs.responses}"
    poll_interval: "${TB_EDQS_POLL_INTERVAL_MS:125}"
    max_pending_requests: "${TB_EDQS_MAX_PENDING_REQUESTS:10000}"
    max_request_timeout: "${TB_EDQS_MAX_REQUEST_TIMEOUT:10000}"
  kafka:
    # Kafka Bootstrap nodes in "host:port" format
    bootstrap.servers: "${TB_KAFKA_SERVERS:localhost:9092}"
    ssl:
      # Enable/Disable SSL Kafka communication
      enabled: "${TB_KAFKA_SSL_ENABLED:false}"
      # The location of the trust store file
      truststore.location: "${TB_KAFKA_SSL_TRUSTSTORE_LOCATION:}"
      # The password of trust store file if specified
      truststore.password: "${TB_KAFKA_SSL_TRUSTSTORE_PASSWORD:}"
      # The location of the key store file. This is optional for the client and can be used for two-way authentication for the client
      keystore.location: "${TB_KAFKA_SSL_KEYSTORE_LOCATION:}"
      # The store password for the key store file. This is optional for the client and only needed if ‘ssl.keystore.location’ is configured. Key store password is not supported for PEM format
      keystore.password: "${TB_KAFKA_SSL_KEYSTORE_PASSWORD:}"
      # The password of the private key in the key store file or the PEM key specified in ‘keystore.key’
      key.password: "${TB_KAFKA_SSL_KEY_PASSWORD:}"
    # The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the durability of records that are sent. The following settings are allowed:0, 1 and all
    acks: "${TB_KAFKA_ACKS:all}"
    # Number of retries. Resend any record whose send fails with a potentially transient error
    retries: "${TB_KAFKA_RETRIES:1}"
    # The compression type for all data generated by the producer. The default is none (i.e. no compression). Valid values none or gzip
    compression.type: "${TB_KAFKA_COMPRESSION_TYPE:none}" # none or gzip
    # Default batch size. This setting gives the upper bound of the batch size to be sent
    batch.size: "${TB_KAFKA_BATCH_SIZE:16384}"
    # This variable creates a small amount of artificial delay—that is, rather than immediately sending out a record
    linger.ms: "${TB_KAFKA_LINGER_MS:1}"
    # The maximum size of a request in bytes. This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests
    max.request.size: "${TB_KAFKA_MAX_REQUEST_SIZE:1048576}"
    # The maximum number of unacknowledged requests the client will send on a single connection before blocking
    max.in.flight.requests.per.connection: "${TB_KAFKA_MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION:5}"
    # The total bytes of memory the producer can use to buffer records waiting to be sent to the server
    buffer.memory: "${TB_BUFFER_MEMORY:33554432}"
    # The multiple copies of data over the multiple brokers of Kafka
    replication_factor: "${TB_QUEUE_KAFKA_REPLICATION_FACTOR:1}"
    # The maximum delay between invocations of poll() method when using consumer group management. This places an upper bound on the amount of time that the consumer can be idle before fetching more records
    max_poll_interval_ms: "${TB_QUEUE_KAFKA_MAX_POLL_INTERVAL_MS:300000}"
    # The maximum number of records returned in a single call of poll() method
    max_poll_records: "${TB_QUEUE_KAFKA_MAX_POLL_RECORDS:8192}"
    # The maximum amount of data per-partition the server will return. Records are fetched in batches by the consumer
    max_partition_fetch_bytes: "${TB_QUEUE_KAFKA_MAX_PARTITION_FETCH_BYTES:16777216}"
    # The maximum amount of data the server will return. Records are fetched in batches by the consumer
    fetch_max_bytes: "${TB_QUEUE_KAFKA_FETCH_MAX_BYTES:134217728}"
    request.timeout.ms: "${TB_QUEUE_KAFKA_REQUEST_TIMEOUT_MS:30000}" # (30 seconds) # refer to https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html#producerconfigs_request.timeout.ms
    session.timeout.ms: "${TB_QUEUE_KAFKA_SESSION_TIMEOUT_MS:10000}" # (10 seconds) # refer to https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html#consumerconfigs_session.timeout.ms
    auto_offset_reset: "${TB_QUEUE_KAFKA_AUTO_OFFSET_RESET:earliest}" # earliest, latest or none
    # Enable/Disable using of Confluent Cloud
    use_confluent_cloud: "${TB_QUEUE_KAFKA_USE_CONFLUENT_CLOUD:false}"
    confluent:
      # The endpoint identification algorithm used by clients to validate server hostname. The default value is https
      ssl.algorithm: "${TB_QUEUE_KAFKA_CONFLUENT_SSL_ALGORITHM:https}"
      # The mechanism used to authenticate Schema Registry requests. SASL/PLAIN should only be used with TLS/SSL as a transport layer to ensure that clear passwords are not transmitted on the wire without encryption
      sasl.mechanism: "${TB_QUEUE_KAFKA_CONFLUENT_SASL_MECHANISM:PLAIN}"
      # Using JAAS Configuration for specifying multiple SASL mechanisms on a broker
      sasl.config: "${TB_QUEUE_KAFKA_CONFLUENT_SASL_JAAS_CONFIG:org.apache.kafka.common.security.plain.PlainLoginModule required username=\"CLUSTER_API_KEY\" password=\"CLUSTER_API_SECRET\";}"
      # Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL
      security.protocol: "${TB_QUEUE_KAFKA_CONFLUENT_SECURITY_PROTOCOL:SASL_SSL}"
    # Key-value properties for Kafka consumer per specific topic, e.g. tb_ota_package is a topic name for ota, tb_rule_engine.sq is a topic name for default SequentialByOriginator queue.
    # Check TB_QUEUE_CORE_OTA_TOPIC and TB_QUEUE_RE_SQ_TOPIC params
    consumer-properties-per-topic:
      tb_ota_package:
        # Key-value properties for Kafka consumer per specific topic, e.g. tb_ota_package is a topic name for ota, tb_rule_engine.sq is a topic name for default SequentialByOriginator queue. Check TB_QUEUE_CORE_OTA_TOPIC and TB_QUEUE_RE_SQ_TOPIC params
        - key: max.poll.records
          # Example of specific consumer properties value per topic
          value: "${TB_QUEUE_KAFKA_OTA_MAX_POLL_RECORDS:10}"
      tb_version_control:
        # Example of specific consumer properties value per topic for VC
        - key: max.poll.interval.ms
          # Example of specific consumer properties value per topic for VC
          value: "${TB_QUEUE_KAFKA_VC_MAX_POLL_INTERVAL_MS:600000}"
    #      tb_rule_engine.sq:
    #        - key: max.poll.records
    #          value: "${TB_QUEUE_KAFKA_SQ_MAX_POLL_RECORDS:1024}"
      tb_housekeeper:
        # Consumer properties for Housekeeper tasks topic
        - key: max.poll.records
          # Amount of records to be returned in a single poll. For Housekeeper tasks topic, we should consume messages (tasks) one by one
          value: "${TB_QUEUE_KAFKA_HOUSEKEEPER_MAX_POLL_RECORDS:1}"
      tb_housekeeper.reprocessing:
        # Consumer properties for Housekeeper reprocessing topic
        - key: max.poll.records
          # Amount of records to be returned in a single poll. For Housekeeper reprocessing topic, we should consume messages (tasks) one by one
          value: "${TB_QUEUE_KAFKA_HOUSEKEEPER_REPROCESSING_MAX_POLL_RECORDS:1}"
    other-inline: "${TB_QUEUE_KAFKA_OTHER_PROPERTIES:}" # In this section you can specify custom parameters (semicolon separated) for Kafka consumer/producer/admin # Example "metrics.recording.level:INFO;metrics.sample.window.ms:30000"
    other: # DEPRECATED. In this section, you can specify custom parameters for Kafka consumer/producer and expose the env variables to configure outside
    #  - key: "request.timeout.ms" # refer to https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html#producerconfigs_request.timeout.ms
    #    value: "${TB_QUEUE_KAFKA_REQUEST_TIMEOUT_MS:30000}" # (30 seconds)
    #  - key: "session.timeout.ms" # refer to https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html#consumerconfigs_session.timeout.ms
    #    value: "${TB_QUEUE_KAFKA_SESSION_TIMEOUT_MS:10000}" # (10 seconds)
    topic-properties:
      # Kafka properties for Rule Engine
      rule-engine: "${TB_QUEUE_KAFKA_RE_TOPIC_PROPERTIES:retention.ms:604800000;segment.bytes:52428800;retention.bytes:1048576000;partitions:1;min.insync.replicas:1}"
      # Kafka properties for Core topics
      core: "${TB_QUEUE_KAFKA_CORE_TOPIC_PROPERTIES:retention.ms:604800000;segment.bytes:52428800;retention.bytes:1048576000;partitions:1;min.insync.replicas:1}"
      # Kafka properties for Transport Api topics
      transport-api: "${TB_QUEUE_KAFKA_TA_TOPIC_PROPERTIES:retention.ms:604800000;segment.bytes:52428800;retention.bytes:1048576000;partitions:10;min.insync.replicas:1}"
      # Kafka properties for Notifications topics
      notifications: "${TB_QUEUE_KAFKA_NOTIFICATIONS_TOPIC_PROPERTIES:retention.ms:604800000;segment.bytes:52428800;retention.bytes:1048576000;partitions:1;min.insync.replicas:1}"
      # Kafka properties for JS Executor topics
      js-executor: "${TB_QUEUE_KAFKA_JE_TOPIC_PROPERTIES:retention.ms:604800000;segment.bytes:52428800;retention.bytes:104857600;partitions:100;min.insync.replicas:1}"
      # Kafka properties for OTA updates topic
      ota-updates: "${TB_QUEUE_KAFKA_OTA_TOPIC_PROPERTIES:retention.ms:604800000;segment.bytes:52428800;retention.bytes:1048576000;partitions:10;min.insync.replicas:1}"
      # Kafka properties for Version Control topic
      version-control: "${TB_QUEUE_KAFKA_VC_TOPIC_PROPERTIES:retention.ms:604800000;segment.bytes:52428800;retention.bytes:1048576000;partitions:1;min.insync.replicas:1}"
      # Kafka properties for Integration Api topics
      integration-api: "${TB_QUEUE_KAFKA_INTEGRATION_TOPIC_PROPERTIES:retention.ms:604800000;segment.bytes:52428800;retention.bytes:1048576000;partitions:10;min.insync.replicas:1}"
      # Kafka properties for Housekeeper tasks topic
      housekeeper: "${TB_QUEUE_KAFKA_HOUSEKEEPER_TOPIC_PROPERTIES:retention.ms:604800000;segment.bytes:52428800;retention.bytes:1048576000;partitions:10;min.insync.replicas:1}"
      # Kafka properties for Housekeeper reprocessing topic; retention.ms is set to 90 days; partitions is set to 1 since only one reprocessing service is running at a time
      housekeeper-reprocessing: "${TB_QUEUE_KAFKA_HOUSEKEEPER_REPROCESSING_TOPIC_PROPERTIES:retention.ms:7776000000;segment.bytes:52428800;retention.bytes:1048576000;partitions:1;min.insync.replicas:1}"
      # Kafka properties for EDQS events topics. Partitions number must be the same as queue.edqs.partitions
      edqs-events: "${TB_QUEUE_KAFKA_EDQS_EVENTS_TOPIC_PROPERTIES:retention.ms:604800000;segment.bytes:52428800;retention.bytes:-1;partitions:12;min.insync.replicas:1}"
      # Kafka properties for EDQS requests topic (default: 3 minutes retention). Partitions number must be the same as queue.edqs.partitions
      edqs-requests: "${TB_QUEUE_KAFKA_EDQS_REQUESTS_TOPIC_PROPERTIES:retention.ms:180000;segment.bytes:52428800;retention.bytes:1048576000;partitions:12;min.insync.replicas:1}"
      # Kafka properties for EDQS state topic (infinite retention, compaction). Partitions number must be the same as queue.edqs.partitions
      edqs-state: "${TB_QUEUE_KAFKA_EDQS_STATE_TOPIC_PROPERTIES:retention.ms:-1;segment.bytes:52428800;retention.bytes:-1;partitions:12;min.insync.replicas:1;cleanup.policy:compact}"
    consumer-stats:
      # Prints lag between consumer group offset and last messages offset in Kafka topics
      enabled: "${TB_QUEUE_KAFKA_CONSUMER_STATS_ENABLED:true}"
      # Statistics printing interval for Kafka's consumer-groups stats
      print-interval-ms: "${TB_QUEUE_KAFKA_CONSUMER_STATS_MIN_PRINT_INTERVAL_MS:60000}"
      # Time to wait for the stats-loading requests to Kafka to finish
      kafka-response-timeout-ms: "${TB_QUEUE_KAFKA_CONSUMER_STATS_RESPONSE_TIMEOUT_MS:1000}"
  partitions:
    hash_function_name: "${TB_QUEUE_PARTITIONS_HASH_FUNCTION_NAME:murmur3_128}" # murmur3_32, murmur3_128 or sha256
  transport_api:
    # Topic used to consume api requests from transport microservices
    requests_topic: "${TB_QUEUE_TRANSPORT_API_REQUEST_TOPIC:tb_transport.api.requests}"
    # Topic used to produce api responses to transport microservices
    responses_topic: "${TB_QUEUE_TRANSPORT_API_RESPONSE_TOPIC:tb_transport.api.responses}"
    # Maximum pending api requests from transport microservices to be handled by server
    max_pending_requests: "${TB_QUEUE_TRANSPORT_MAX_PENDING_REQUESTS:10000}"
    # Maximum timeout in milliseconds to handle api request from transport microservice by server
    max_requests_timeout: "${TB_QUEUE_TRANSPORT_MAX_REQUEST_TIMEOUT:10000}"
    # Amount of threads used to invoke callbacks
    max_callback_threads: "${TB_QUEUE_TRANSPORT_MAX_CALLBACK_THREADS:100}"
    # Amount of threads used for transport API requests
    max_core_handler_threads: "${TB_QUEUE_TRANSPORT_MAX_CORE_HANDLER_THREADS:16}"
    # Interval in milliseconds to poll api requests from transport microservices
    request_poll_interval: "${TB_QUEUE_TRANSPORT_REQUEST_POLL_INTERVAL_MS:25}"
    # Interval in milliseconds to poll api response from transport microservices
    response_poll_interval: "${TB_QUEUE_TRANSPORT_RESPONSE_POLL_INTERVAL_MS:25}"
  core:
    # Default topic name of Kafka, RabbitMQ, etc. queue
    topic: "${TB_QUEUE_CORE_TOPIC:tb_core}"
    # Interval in milliseconds to poll messages by Core microservices
    poll-interval: "${TB_QUEUE_CORE_POLL_INTERVAL_MS:25}"
    # Amount of partitions used by Core microservices
    partitions: "${TB_QUEUE_CORE_PARTITIONS:10}"
    # Timeout for processing a message pack by Core microservices
    pack-processing-timeout: "${TB_QUEUE_CORE_PACK_PROCESSING_TIMEOUT_MS:2000}"
    # Enable/disable a separate consumer per partition for Core queue
    consumer-per-partition: "${TB_QUEUE_CORE_CONSUMER_PER_PARTITION:true}"
    ota:
      # Default topic name for OTA updates
      topic: "${TB_QUEUE_CORE_OTA_TOPIC:tb_ota_package}"
      # The interval of processing the OTA updates for devices. Used to avoid any harm to the network due to many parallel OTA updates
      pack-interval-ms: "${TB_QUEUE_CORE_OTA_PACK_INTERVAL_MS:60000}"
      # The size of OTA updates notifications fetched from the queue. The queue stores pairs of firmware and device ids
      pack-size: "${TB_QUEUE_CORE_OTA_PACK_SIZE:100}"
    # Stats topic name for queue Kafka, RabbitMQ, etc.
    usage-stats-topic: "${TB_QUEUE_US_TOPIC:tb_usage_stats}"
    stats:
      # Enable/disable statistics for Core microservices
      enabled: "${TB_QUEUE_CORE_STATS_ENABLED:true}"
      # Statistics printing interval for Core microservices
      print-interval-ms: "${TB_QUEUE_CORE_STATS_PRINT_INTERVAL_MS:60000}"
    housekeeper:
      # Topic name for Housekeeper tasks
      topic: "${TB_HOUSEKEEPER_TOPIC:tb_housekeeper}"
      # Topic name for Housekeeper tasks to be reprocessed
      reprocessing-topic: "${TB_HOUSEKEEPER_REPROCESSING_TOPIC:tb_housekeeper.reprocessing}"
      # Poll interval for topics related to Housekeeper
      poll-interval-ms: "${TB_HOUSEKEEPER_POLL_INTERVAL_MS:500}"
      # Timeout in milliseconds for task processing. Tasks that fail to finish on time will be submitted for reprocessing
      task-processing-timeout-ms: "${TB_HOUSEKEEPER_TASK_PROCESSING_TIMEOUT_MS:120000}"
      # Comma-separated list of task types that shouldn't be processed. Available task types:
      # DELETE_ATTRIBUTES, DELETE_TELEMETRY (both DELETE_LATEST_TS and DELETE_TS_HISTORY will be disabled),
      # DELETE_LATEST_TS, DELETE_TS_HISTORY, DELETE_EVENTS, DELETE_ALARMS, UNASSIGN_ALARMS
      disabled-task-types: "${TB_HOUSEKEEPER_DISABLED_TASK_TYPES:}"
      # Delay in milliseconds between tasks reprocessing
      task-reprocessing-delay-ms: "${TB_HOUSEKEEPER_TASK_REPROCESSING_DELAY_MS:3000}"
      # Maximum amount of task reprocessing attempts. After exceeding, the task will be dropped
      max-reprocessing-attempts: "${TB_HOUSEKEEPER_MAX_REPROCESSING_ATTEMPTS:10}"
      stats:
        # Enable/disable statistics for Housekeeper
        enabled: "${TB_HOUSEKEEPER_STATS_ENABLED:true}"
        # Statistics printing interval for Housekeeper
        print-interval-ms: "${TB_HOUSEKEEPER_STATS_PRINT_INTERVAL_MS:60000}"

  vc:
    # Default topic name for Kafka, RabbitMQ, etc.
    topic: "${TB_QUEUE_VC_TOPIC:tb_version_control}"
    # Number of partitions to associate with this queue. Used for scaling the number of messages that can be processed in parallel
    partitions: "${TB_QUEUE_VC_PARTITIONS:10}"
    # Interval in milliseconds between polling of the messages if no new messages arrive
    poll-interval: "${TB_QUEUE_VC_INTERVAL_MS:25}"
    # Timeout before retrying all failed and timed-out messages from the processing pack
    pack-processing-timeout: "${TB_QUEUE_VC_PACK_PROCESSING_TIMEOUT_MS:180000}"
    # Timeout for a request to VC-executor (for a request for the version of the entity, for a commit charge, etc.)
    request-timeout: "${TB_QUEUE_VC_REQUEST_TIMEOUT:180000}"
    # Queue settings for Kafka, RabbitMQ, etc. Limit for single message size
    msg-chunk-size: "${TB_QUEUE_VC_MSG_CHUNK_SIZE:250000}"
  js:
    # JS Eval request topic
    request_topic: "${REMOTE_JS_EVAL_REQUEST_TOPIC:js_eval.requests}"
    # JS Eval responses topic prefix that is combined with node id
    response_topic_prefix: "${REMOTE_JS_EVAL_RESPONSE_TOPIC:js_eval.responses}"
    # JS Eval max pending requests
    max_pending_requests: "${REMOTE_JS_MAX_PENDING_REQUESTS:10000}"
    # JS Eval max request timeout
    max_eval_requests_timeout: "${REMOTE_JS_MAX_EVAL_REQUEST_TIMEOUT:60000}"
    # JS max request timeout
    max_requests_timeout: "${REMOTE_JS_MAX_REQUEST_TIMEOUT:10000}"
    # JS execution max request timeout
    max_exec_requests_timeout: "${REMOTE_JS_MAX_EXEC_REQUEST_TIMEOUT:2000}"
    # JS response poll interval
    response_poll_interval: "${REMOTE_JS_RESPONSE_POLL_INTERVAL_MS:25}"
  rule-engine:
    # Deprecated. It will be removed in the nearest releases
    topic: "${TB_QUEUE_RULE_ENGINE_TOPIC:tb_rule_engine}"
    # Interval in milliseconds to poll messages by Rule Engine
    poll-interval: "${TB_QUEUE_RULE_ENGINE_POLL_INTERVAL_MS:25}"
    # Timeout for processing a message pack of Rule Engine
    pack-processing-timeout: "${TB_QUEUE_RULE_ENGINE_PACK_PROCESSING_TIMEOUT_MS:2000}"
    stats:
      # Enable/disable statistics for Rule Engine
      enabled: "${TB_QUEUE_RULE_ENGINE_STATS_ENABLED:true}"
      # Statistics printing interval for Rule Engine
      print-interval-ms: "${TB_QUEUE_RULE_ENGINE_STATS_PRINT_INTERVAL_MS:60000}"
      # Max length of the error message that is printed by statistics
      max-error-message-length: "${TB_QUEUE_RULE_ENGINE_MAX_ERROR_MESSAGE_LENGTH:4096}"
    # After a queue is deleted (or the profile's isolation option was disabled), Rule Engine will continue reading related topics during this period before deleting the actual topics
    topic-deletion-delay: "${TB_QUEUE_RULE_ENGINE_TOPIC_DELETION_DELAY_SEC:15}"
    # Size of the thread pool that handles such operations as partition changes, config updates, queue deletion
    management-thread-pool-size: "${TB_QUEUE_RULE_ENGINE_MGMT_THREAD_POOL_SIZE:12}"
  transport:
    # For high-priority notifications that require minimum latency and processing time
    notifications_topic: "${TB_QUEUE_TRANSPORT_NOTIFICATIONS_TOPIC:tb_transport.notifications}"
    # Interval in milliseconds to poll messages
    poll_interval: "${TB_QUEUE_TRANSPORT_NOTIFICATIONS_POLL_INTERVAL_MS:25}"
  integration:
    # Name of hash function used for consistent hash ring in Cluster Mode. See architecture docs for more details. Valid values - murmur3_32, murmur3_128 or sha256
    partitions: "${TB_QUEUE_INTEGRATION_PARTITIONS:3}"
    # Default notification topic name used by queue
    notifications_topic: "${TB_QUEUE_INTEGRATION_NOTIFICATIONS_TOPIC:tb_ie.notifications}"
    # Default downlink topic name used by queue
    downlink_topic: "${TB_QUEUE_INTEGRATION_DOWNLINK_TOPIC:tb_ie.downlink}"
    # Default uplink topic name used by queue
    uplink_topic: "${TB_QUEUE_INTEGRATION_UPLINK_TOPIC:tb_ie.uplink}"
    # Interval in milliseconds to poll messages by integrations
    poll_interval: "${TB_QUEUE_INTEGRATION_POLL_INTERVAL_MS:25}"
    # Timeout for processing a message pack by integrations
    pack-processing-timeout: "${TB_QUEUE_INTEGRATION_PACK_PROCESSING_TIMEOUT_MS:10000}"
  integration_api:
    # Default Integration Api request topic name used by queue
    requests_topic: "${TB_QUEUE_INTEGRATION_EXECUTOR_API_REQUEST_TOPIC:tb_ie.api.requests}"
    # Default Integration Api response topic name used by queue
    responses_topic: "${TB_QUEUE_INTEGRATION_EXECUTOR_API_RESPONSE_TOPIC:tb_ie.api.responses}"
    # Maximum pending api requests from integration executor to be handled by server<
    max_pending_requests: "${TB_QUEUE_INTEGRATION_EXECUTOR_MAX_PENDING_REQUESTS:10000}"
    # Maximum timeout in milliseconds to handle api request from integration executor microservice by server
    max_requests_timeout: "${TB_QUEUE_INTEGRATION_EXECUTOR_MAX_REQUEST_TIMEOUT:10000}"
    # Amount of threads used to invoke callbacks
    max_callback_threads: "${TB_QUEUE_INTEGRATION_EXECUTOR_MAX_CALLBACK_THREADS:10}"
    # Interval in milliseconds to poll api requests from integration executor microservices
    request_poll_interval: "${TB_QUEUE_INTEGRATION_EXECUTOR_REQUEST_POLL_INTERVAL_MS:25}"
    # Interval in milliseconds to poll api response from integration executor microservices
    response_poll_interval: "${TB_QUEUE_INTEGRATION_EXECUTOR_RESPONSE_POLL_INTERVAL_MS:25}"

# General service parameters
service:
  type: "${TB_SERVICE_TYPE:edqs}"
  # Unique id for this service (autogenerated if empty)
  id: "${TB_SERVICE_ID:}"
  edqs:
    label: "${TB_EDQS_LABEL:}" # services with the same label will share the list of partitions

# Metrics parameters
metrics:
  # Enable/disable actuator metrics.
  enabled: "${METRICS_ENABLED:false}"
  timer:
    # Metrics percentiles returned by actuator for timer metrics. List of double values (divided by ,).
    percentiles: "${METRICS_TIMER_PERCENTILES:0.5}"
  system_info:
    # Persist frequency of system info (CPU, memory usage, etc.) in seconds
    persist_frequency: "${METRICS_SYSTEM_INFO_PERSIST_FREQUENCY_SECONDS:60}"
    # TTL in days for system info timeseries
    ttl: "${METRICS_SYSTEM_INFO_TTL_DAYS:7}"

# General management parameters
management:
  endpoints:
    web:
      exposure:
        # Expose metrics endpoint (use value 'prometheus' to enable prometheus metrics).
        include: '${METRICS_ENDPOINTS_EXPOSE:info}'
  health:
    elasticsearch:
      # Enable the org.springframework.boot.actuate.elasticsearch.ElasticsearchRestClientHealthIndicator.doHealthCheck
      enabled: "false"
